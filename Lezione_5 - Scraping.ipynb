{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2d93e5",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0546f",
   "metadata": {},
   "source": [
    "Lo faremo con 3 Metodi:\n",
    "- Scraping tramite requests della main page + parsing html (tramite BeautifulSoup).\n",
    "- Scraping tramite selenium.\n",
    "- Chiamate API (post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29bc5e9",
   "metadata": {},
   "source": [
    "## Requests + BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc04dd0",
   "metadata": {},
   "source": [
    "Proviamo a fare scraping di qualche video su Immobiliare.it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf317ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup   # Da installare con pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f33b1",
   "metadata": {},
   "source": [
    "Per prima cosa bisogna analizzare il sito manualmente con \"Chrome DevTools\". Obiettivo:\n",
    "- Cercare le macro-schede che possano ricondurre a tutti i dati all'interno di _____ (se possibile)\n",
    "- Identificare le query css selector sia per l'intera scheda sia per i dati al suo interno.\n",
    "Man mano che abbiamo pianificato quali query useremo, iniziamo a scrivercele in delle variabili qui sotto e poi testiamo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_css = 'pre[class=\"lang-py s-code-block\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702f197",
   "metadata": {},
   "source": [
    "1. Usiamo requests.get(url) per richiedere al server di fornirci l'html della pagina web.\n",
    "2. Ora abbiamo la sua risposta ma l'html fornito è una semplice enorme stringa, difficile da utilizzare per ricercare con le nostre query selector. Per questo motivo usiamo BeautifulSoup che è un \"parser\" in grado di riconoscere del codice html da una stringa e lo struttura per noi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e002d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://stackoverflow.com/questions/2612548/extracting-an-attribute-value-with-beautifulsoup\"\n",
    "\n",
    "headers= {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36\"\n",
    "}\n",
    "risposta = requests.get(url, headers=headers)\n",
    "main_page = BeautifulSoup(risposta.text, features=\"html.parser\")\n",
    "print(main_page.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a7085",
   "metadata": {},
   "source": [
    "Ora che abbiamo la pagina web \"parsificata\" possiamo utilizzare dei metodi che BeautifulSoup ci mette a disposizione per ricercare all'interno dell'html.  \n",
    "Uno dei più importanti per il nostro scopo è: .select() e .select_one().  \n",
    "Select ci permette di inserire una query css selector da ricercare. Select_one cerca solo la prima occorrenza e ci restituisce solo quella, mentre select ce le mette tutte quelle che trova in una lista.  \n",
    "N.B: Questa cosa è importante perché:\n",
    "- **select_one** -> Restituisce un oggetto: \"Tag\"\n",
    "- **select** -> Restituisce un oggetto: \"ResultSet[Tag]\", che è una specie di lista di Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca20225",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = main_page.select('pre[class=\"lang-py s-code-block\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3be1a2",
   "metadata": {},
   "source": [
    "## Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   # Usato solo per ricavare il percorso assoluto della cartella attuale del progetto\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7e9de",
   "metadata": {},
   "source": [
    "Iniziamo ad impostare alcune opzioni per il webdriver di chrome:  \n",
    "- **headless** -> Modalità in cui non viene visualizzata la finestra del browser. Consiglio: durante la fase di test e di costruzione dello scraper, meglio non attivare questa modalità in modo da monitorare con esattezza quello che sta accadendo. Una volta ben testato si può aggiungere e far andare lo script in \"background\".  \n",
    "- **disable-gpu** -> Tipicamente usa su windows + modalità \"headless. Evita eventuali problemi dati dal tentativo di chrome di caricare la grafica anche se non dovrebbe farlo dato che abbiamo specificato \"headless\".  \n",
    "- **window-size=1920,1080** -> Si imposta una dimensione della finestra. Impostiamo anche se siamo in modalità \"headless\" perché: anche se noi non la vediamo, è come se la finestra venga comunque creata e dare una dimensione fissa in cui sappiamo in anticipo quali oggetti vengono visualizzati con quella precisa dimensione ci permette di replicare con esattezza alcune situazioni e azioni (esempio, sappiamo che per trovare un pulsante non dobbiamo fare altro che attendere che la pagina venga caricata ma se la finestra viene creata più piccola, il pulsante non è visibile e necessiterebbe di uno \"scroll\" verso il basso!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--headless\")              \n",
    "options.add_argument(\"--disable-gpu\")           \n",
    "options.add_argument(\"--window-size=1920,1080\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f9cca5",
   "metadata": {},
   "source": [
    "Qui abbiamo delle opzioni \"extra\". Non vi capiterà spesso di usarle ma in questo caso è utile:\n",
    "- **add_experimental_option** -> Sono delle opzioni particolari, in questo caso aggiungiamo:\n",
    "    - **download.default_directory** -> Ci permette di scegliere una cartella specifica sul pc dove i file scaricati dal nostro webdriver (il nostro falso browser) debbano essere scaricati in automatico (anziché la classica cartella \"download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefs = {\"download.default_directory\": os.path.abspath(\".\")}\n",
    "options.add_experimental_option(\"prefs\", prefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4ab477",
   "metadata": {},
   "source": [
    "Creiamo il nostro driver con le nostre opzioni.  \n",
    "Si può creare il webdriver senza specificare nessuna opzione se non ci servono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa438744",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4afca",
   "metadata": {},
   "source": [
    "Ora, in modo simile a requests, effettuiamo il .get() della pagina web.  \n",
    "Quello che succede è che il driver effettua la \"richiesta\" dell'html della pagina che gli viene restituita e lui, proprio come fa normalmente un browser, la visualizza e ovviamente la tiene in memoria dentro di sé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://chatgpt.it/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888981c7",
   "metadata": {},
   "source": [
    "Ora procediamo a trovare i vari elementi che ci servono e interagiamoci proprio come faremmo normalemente con un browser.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6765553",
   "metadata": {},
   "source": [
    "Usiamo il primo come esempio per spiegare come si procede:\n",
    "- **try / except** -> Sono dei comandi python che servono a impedire il crash di python se questo avviene all'interno del try.\n",
    "In pratica si sta \"avvertendo\" python che: PROVA ad eseguire questo codice... so già che potrebbe \"alzare un eccezione\" (raise exeption) che solitamente blocca l'esecuzione del programma. Ma visto che ti ho avvertito che questo può succedere, anziché bloccare il programma, esegui il codice in except e continua normalmente. Questo viene fatto perché il metodo di webdriver che utilizzeremo, se non trova l'elemento richiesto è programmato per dare un eccezione ma visto che non sappiamo se un elemento verrà visualizzato con certezza o meno (esempio accettare i coockies di un sito che a volte viene visualizzato e a volte no), gli impediamo di crashare.\n",
    "- **WebDriverWait** -> Dobbiamo dargli il nostro driver come parametro e un intero che corrisponde ai secondi che deve aspettare al massimo prima di arrendersi. Usiamo questa classe per dare tempo fisico al browser di visualizzare l'elemento che vogliamo cercare. Si potrebbe risolvere anche usando la libreria di python **import time** e usando il comando \"time.sleep(n_secondi)\" ma in quel caso aspetterebbe un numero di secondi preciso sempre mentre tramite questa sua classe, selenium aspetta massimo il numero di secondi indicato ma se lo trova prima va avanti, quindi è più efficiente.\n",
    "    - **.until** -> Questo metodo di WebDriverWait ci permette di aspettare \"finché\" non avviene un qualcosa che specificheremo tramite parametro (esiste anche il .untilnot()).  \n",
    "- **EC** -> Quest'altra classe ci serve invece per dare una \"expected_conditions\" quindi possiamo specificare una serie di condizioni da aspettare che si verifichino. Tipicamente usiamo:\n",
    "    - **.visibility_of_element_located()** -> Che è proprio la funzione che specifica che la condizione da aspettare è che l'elemento, che richiederemo nei suoi parametri, sia visibile nel browser.\n",
    "- **By** -> E' invece la classe che ci permette di specificare il tipo di \"query\" da utilizzare per cercare l'elemento. In questo caso specifichiamo:\n",
    "    - **.CSS_SELECTOR** -> E quindi la query selector che abbiamo già trovato tramite \"F12\" su google chrome.  \n",
    "\n",
    "Una volta trovato l'elemento, in base al tipo di elemento trovato si possono eseguire delle azioni.  \n",
    "In questo caso, abbiamo trovato un \"button\" e quindi possiamo eseguire il comando **.click()** che simula il click del mouse sul tasto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e53f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    button_coockies = WebDriverWait(driver, 10).until(EC.element_to_be_clickable(\n",
    "        (By.CSS_SELECTOR, '.fc-button.fc-cta-consent'))).click()\n",
    "except:\n",
    "    print(\"Non ho trovato schermata coockies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea17aed",
   "metadata": {},
   "source": [
    "Capito il primo ora è facile intuire anche tutti gli altri.  \n",
    "In sostanza quello che abbiamo scritto di fare è:\n",
    "- Trova e inserisci testo nel box in cui si può digitare il messaggio (.send_keys(message)).  \n",
    "- Trova e clicca il tasto per dare l'ok (.click()).  \n",
    "- Trova e clicca il tasto per scaricare la conversazione sotto forma di .txt (.click())\n",
    "Il tutto dentro il nostro try per evitare il crash del programma.  \n",
    "Sarebbe più corretto inserire ogni comando all'interno di un proprio try in modo da avere un print specifico per ogni problema che può venir fuori dallo scraper. In altri casi si potrebbe decidere di non usare il try perché quella operazione è imperativa per far proseguire il programma quindi decidiamo di farlo crashare se non riesce ad eseguirla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86346af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    text_box = WebDriverWait(driver, 10).until(EC.visibility_of_element_located(\n",
    "        (By.CSS_SELECTOR, '.auto-expand.wpaicg-chat-shortcode-typing'))).send_keys(\"Che tempo farà domani?\")\n",
    "\n",
    "    button = WebDriverWait(driver, 10).until(EC.visibility_of_element_located(\n",
    "        (By.CSS_SELECTOR, 'span[class=\"wpaicg-chat-shortcode-send\"]'))).click()\n",
    "    time.sleep(1)\n",
    "    download_button = WebDriverWait(driver, 10).until(EC.visibility_of_element_located(\n",
    "        (By.CSS_SELECTOR, 'span[class=\"wpaicg-chatbox-download-btn\"]'))).click()\n",
    "except:\n",
    "    print(\"Qualcosa è andato storto!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55fe990",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b48be",
   "metadata": {},
   "source": [
    "In realtà si possono effettuare queste operazioni anche senza dover usare \"WebDriverWait\" in modo più semplice:\n",
    "- .find_element -> Cerca un elemento tramite:\n",
    "    - By.CSS_SELECTOR\n",
    "    - O altri metodi di query (Esempio XPATH)\n",
    "In questo caso funziona comunque ma ricordarsi che se la pagina è lenta a essere caricata e gli elementi non sono immediatamente visibili, find_element potrebbe non trovare l'elemento e crashare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc43f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    text_box = driver.find_element(By.CSS_SELECTOR, '.auto-expand.wpaicg-chat-shortcode-typing').send_keys(\"Che tempo farà domani?\")\n",
    "    button = driver.find_element(By.CSS_SELECTOR, 'span[class=\"wpaicg-chat-shortcode-send\"]').click()\n",
    "    download_button = driver.find_element(By.CSS_SELECTOR, 'span[class=\"wpaicg-chatbox-download-btn\"]').click()\n",
    "except:\n",
    "    print(\"Qualcosa è andato storto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82dff9",
   "metadata": {},
   "source": [
    "E' possibile cercare più elementi contemporaneamente tramite:\n",
    "- .find_elements -> In questo caso fare attenzione al fatto che la funzione restituisce una lista di elementi (anche se ne trova solo 1!)\n",
    "oppure nel caso di EC.visibility_of_all_elements_located (elements!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c132e9",
   "metadata": {},
   "source": [
    "A partire da un elemento che viene trovato è possibile continuare ad utilizzarlo per cercarne degli altri al suo interno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdfed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo a scopo illustrativo, non abbiamo fatto il .get() della pagina tweetter\n",
    "try:\n",
    "    tweets = WebDriverWait(driver, 10).until(\n",
    "                    EC.visibility_of_all_elements_located((By.CSS_SELECTOR, 'article')))    # elements!\n",
    "except:\n",
    "    print(\"No tweets trovati!\")\n",
    "\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        time_stamp = tweet.find_element(By.CSS_SELECTOR, 'time').get_attribute('datetime')\n",
    "        url_tweet = tweet.find_element(By.CSS_SELECTOR,\n",
    "                'a[style=\"color: rgb(113, 118, 123);\"]').get_attribute(\"href\")\n",
    "    except:\n",
    "        print(\"Info tweet non trovati\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4cf474",
   "metadata": {},
   "source": [
    "## API Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Ciao ti scrivo da python! Ti saluta requests e BeautifulSoup!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b4c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ciao Python! Che bello sentirti! Requests e BeautifulSoup sono due strumenti fantastici per il web scraping e l'estrazione di dati. Se hai bisogno di aiuto con qualche codice o vuoi sapere come usarli al meglio, sono qui per te! 😊\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = \"https://chatgpt.it/wp-admin/admin-ajax.php\"\n",
    "\n",
    "payload = {\n",
    "    \"_wpnonce\": \"5b7aac3794\",\n",
    "    \"post_id\": \"106\",\n",
    "    \"url\": \"https://chatgpt.it\",\n",
    "    \"action\": \"wpaicg_chat_shortcode_message\",\n",
    "    \"message\": message,\n",
    "    \"bot_id\": \"0\",\n",
    "    \"chatbot_identity\": \"shortcode\",\n",
    "    \"wpaicg_chat_history\": '[]',\n",
    "    \"wpaicg_chat_client_id\":\"8FQLM3hp6Q\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Origin\": \"https://chatgpt.it\",\n",
    "    \"Referer\": \"https://chatgpt.it/\",\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "}\n",
    "response = requests.post(url, data=payload, headers=headers)\n",
    "\n",
    "re_json = response.json()\n",
    "re_json[\"data\"]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
